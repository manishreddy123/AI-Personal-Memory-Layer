
# üß† AI Personal Memory Layer

This project is a **local AI-powered memory assistant** that integrates your **Gmail**, **Notion**, and **Google Calendar** data into a unified searchable memory. It uses **RAG (Retrieval-Augmented Generation)** with **local LLMs (via Ollama)** to answer questions about your digital life‚Äîfully on your machine.

---

## ‚ú® Features

- üìß Retrieve recent Gmail emails
- üìÖ Extract events from Google Calendar
- üóÉÔ∏è Load structured notes from Notion
- üß† Build a vector-based memory using FAISS
- üí¨ Ask natural language questions about your own data
- üîí 100% private, runs locally using **Ollama**

---

## ‚öôÔ∏è Technologies Used

- üß† LangChain + FAISS for RAG
- üóÉÔ∏è Notion API
- üìß Gmail + Google Calendar API
- üß™ Local LLMs via [Ollama](https://ollama.com/)
- üê≥ Docker (optional)
- üêç Python 3.9+

---

## üìÅ Project Structure

```
personal_memory_ai/
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ loaders/
‚îÇ   ‚îú‚îÄ‚îÄ gmail_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ calendar_loader.py
‚îÇ   ‚îî‚îÄ‚îÄ notion_loader.py
‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py
‚îÇ   ‚îî‚îÄ‚îÄ rag_chain.py
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îî‚îÄ‚îÄ memory_agent.py
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ chat_terminal.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üöÄ How to Run This Project

### 1. ‚úÖ Prerequisites

- Python 3.9+
- `pip install -r requirements.txt`
- [Install Ollama](https://ollama.com/)

---

## üîß Setting Up Google Cloud APIs (Gmail & Calendar)

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project (e.g., `Personal Memory`)
3. Navigate to **APIs & Services > Library**
   - Enable **Gmail API** and **Google Calendar API**
4. Go to **APIs & Services > OAuth consent screen**
   - User type: `External`
   - Fill required fields
   - Add your Google account as a **test user**
5. Go to **Credentials > Create Credentials > OAuth Client ID**
   - Application type: `Desktop App`
   - Download the `credentials.json`
6. Place `credentials.json` in your project root

---

## üóÉÔ∏è Setting Up Notion API

1. Visit [https://www.notion.so/my-integrations](https://www.notion.so/my-integrations)
2. Click **+ New integration**
   - Name: `Personal Memory AI`
   - Capabilities: Check `Read content`
   - Submit and **copy your Internal Integration Token**
3. In Notion, open the database (table view)
   - Click **Share**
   - Invite your integration
4. Copy the database ID from the URL
   ```
   https://www.notion.so/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   ```
   Use the first 32-character string as `NOTION_DB_ID`

---

## ü¶ô Setting Up Ollama & LLM (LLaMA)

1. Download Ollama from [https://ollama.com/download](https://ollama.com/download)
2. Install and restart terminal
3. Start Ollama server:
   ```bash
   ollama serve
   ```
4. Download and start a model (e.g., LLaMA 3):
   ```bash
   ollama run llama3
   ```

---

## üõ† Create `.env` File

```env
NOTION_API_KEY=secret_xxxxxxxxxxxxxxxxxxxxxxx
NOTION_DB_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GOOGLE_CLIENT_SECRET_FILE=credentials.json
```

---

## üß™ Run the App

```bash
python main.py
```

- Authorize access to Gmail and Calendar in browser
- Data will be loaded into local vectorstore
- Chat with your memory

---

## üí¨ Example Queries

```
What was my last email about?
What‚Äôs on my calendar this week?
Summarize my recent Notion project notes.
```

---

## üê≥ Docker Option

```bash
docker-compose build
docker-compose up
```

---

## üß† LLM Integration (Ollama)

Use Ollama in LangChain like this:

```python
from langchain_community.llms import Ollama
llm = Ollama(model="llama3")
```

---

## üìå Notes

- You can expand support to local PDFs, TXT, Markdown files
- Ollama must be running in the background
- Add error handling or UI using Streamlit optionally

---

